$ hadoop jar units.jar org.mw.bigdata.hadoop.ProcessUnits input_dir output_dir
17/03/25 23:26:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
17/03/25 23:26:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
17/03/25 23:26:35 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
17/03/25 23:26:35 INFO mapred.FileInputFormat: Total input paths to process : 1
17/03/25 23:26:35 INFO mapreduce.JobSubmitter: number of splits:2
17/03/25 23:26:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490500543231_0008
17/03/25 23:26:35 INFO impl.YarnClientImpl: Submitted application application_1490500543231_0008
17/03/25 23:26:35 INFO mapreduce.Job: The url to track the job: http://ubuntu:8088/proxy/application_1490500543231_0008/
17/03/25 23:26:35 INFO mapreduce.Job: Running job: job_1490500543231_0008
17/03/25 23:26:41 INFO mapreduce.Job: Job job_1490500543231_0008 running in uber mode : false
17/03/25 23:26:41 INFO mapreduce.Job:  map 0% reduce 0%
17/03/25 23:26:45 INFO mapreduce.Job:  map 50% reduce 0%
17/03/25 23:26:46 INFO mapreduce.Job:  map 100% reduce 0%
17/03/25 23:26:50 INFO mapreduce.Job:  map 100% reduce 100%
17/03/25 23:26:50 INFO mapreduce.Job: Job job_1490500543231_0008 completed successfully
17/03/25 23:26:50 INFO mapreduce.Job: Counters: 49
    File System Counters
        FILE: Number of bytes read=39
        FILE: Number of bytes written=356684
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=726
        HDFS: Number of bytes written=24
        HDFS: Number of read operations=9
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters 
        Launched map tasks=2
        Launched reduce tasks=1
        Data-local map tasks=2
        Total time spent by all maps in occupied slots (ms)=5349
        Total time spent by all reduces in occupied slots (ms)=2048
        Total time spent by all map tasks (ms)=5349
        Total time spent by all reduce tasks (ms)=2048
        Total vcore-milliseconds taken by all map tasks=5349
        Total vcore-milliseconds taken by all reduce tasks=2048
        Total megabyte-milliseconds taken by all map tasks=5477376
        Total megabyte-milliseconds taken by all reduce tasks=2097152
    Map-Reduce Framework
        Map input records=5
        Map output records=5
        Map output bytes=45
        Map output materialized bytes=45
        Input split bytes=216
        Combine input records=5
        Combine output records=3
        Reduce input groups=3
        Reduce shuffle bytes=45
        Reduce input records=3
        Reduce output records=3
        Spilled Records=6
        Shuffled Maps =2
        Failed Shuffles=0
        Merged Map outputs=2
        GC time elapsed (ms)=166
        CPU time spent (ms)=2150
        Physical memory (bytes) snapshot=730624000
        Virtual memory (bytes) snapshot=5741301760
        Total committed heap usage (bytes)=552075264
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=510
    File Output Format Counters 
        Bytes Written=24

$ hadoop fs -ls output_dir
Found 2 items
-rw-r--r--   1 moonwave supergroup          0 2017-03-25 23:26 output_dir/_SUCCESS
-rw-r--r--   1 moonwave supergroup         24 2017-03-25 23:26 output_dir/part-00000

$ hadoop fs -cat output_dir/part-00000
1981    34
1984    40
1985    45

Note: only print average value > 30 years
      see 
      int maxavg=30;
      defined in PreocessUnits::reduce() line 84

